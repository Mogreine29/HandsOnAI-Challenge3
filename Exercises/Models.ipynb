{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import os, glob\n",
    "import keras\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [20, 10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 possible datas : ../data/public/train.csv and ../data/public/clean_train.csv\n",
    "\n",
    "DT_train = pd.read_csv(\"../data/public/train.csv\", parse_dates=True)\n",
    "DT_train[\"Day\"] = pd.to_datetime(DT_train[\"Day\"], format=\"%Y-%m-%d\")\n",
    "DT_train.set_index(\"Day\", inplace=True)\n",
    "DT_train = DT_train.asfreq(\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filling missing values if using the original data\n",
    "DT_train.fillna(method=\"backfill\", inplace=True)\n",
    "DT_train.isna().any().any()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Function for kaggle submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Dataframe (HORIZON X SERIES) to Kaggle format (s001h3, ...)\n",
    "def kaggle_forecasts(fcts):\n",
    "    submission = fcts.copy()\n",
    "    submission.index = 1 + np.arange(len(submission))\n",
    "    submission = submission.stack()\n",
    "    submission.name = \"Forecasts\"\n",
    "    submission = submission.reset_index()\n",
    "\n",
    "    submission[\"Id\"] = submission[\"level_1\"] + \"h\" + submission[\"level_0\"].apply(str)\n",
    "    submission.drop([\"level_0\", \"level_1\"], axis=1, inplace=True)\n",
    "    submission = submission[[\"Id\", \"Forecasts\"]]\n",
    "    return submission"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Define the Horizon of forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "HORIZON = 7 * 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`forecast_for_kaggle` has to be set to True for the Kaggle competition.\n",
    "\n",
    "Setting it to False allows to have access to a test dataset using data from `DT_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting dates for validation and test data\n",
    "valid_start_dt = \"1997-09-20\"\n",
    "\n",
    "forecast_for_kaggle = True\n",
    "if forecast_for_kaggle:\n",
    "    test_start_dt = \"1998-03-23\"\n",
    "else:\n",
    "    test_start_dt = dt.datetime.strptime(\"1998-03-23\", \"%Y-%m-%d\") - dt.timedelta(days=HORIZON)\n",
    "    test_start_dt = test_start_dt.strftime(\"%Y-%m-%d\")\n",
    "    DT_test = DT_train.tail(HORIZON).copy()\n",
    "    DT_train = DT_train.head(-HORIZON).copy()\n",
    "\n",
    "last_day_train = DT_train.index[-1]\n",
    "test_dates = pd.date_range(start=last_day_train, periods=HORIZON + 1)[1:]\n",
    "\n",
    "\n",
    "# `use_subset_of_series` restricts the number of series to 2 for faster results.\n",
    "use_subset_of_series = False\n",
    "if use_subset_of_series:\n",
    "    id_series_all = DT_train.columns[:2]\n",
    "else:\n",
    "    id_series_all = DT_train.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonal naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "period = 28\n",
    "T = len(DT_train)\n",
    "\n",
    "fcts_snaive_list = list()\n",
    "\n",
    "for id_series in id_series_all:\n",
    "    series_train = DT_train[id_series]\n",
    "    f_snaive = [series_train[T + h - period * ((HORIZON - 1) // period + 1)] for h in range(0, HORIZON)]\n",
    "    f_snaive = pd.Series(f_snaive, index=test_dates)\n",
    "    f_snaive.name = id_series\n",
    "    fcts_snaive_list.append(f_snaive)\n",
    "\n",
    "fcts_snaive = pd.concat(fcts_snaive_list, axis=1)\n",
    "\n",
    "kaggle_submission_naive = kaggle_forecasts(fcts_snaive)\n",
    "kaggle_submission_naive.to_csv(\"../work/submission_snaive.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto Arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== s001 ======\n",
      "====== s002 ======\n",
      "====== s003 ======\n",
      "====== s004 ======\n",
      "====== s005 ======\n",
      "====== s006 ======\n",
      "====== s007 ======\n",
      "====== s008 ======\n",
      "====== s009 ======\n",
      "====== s010 ======\n",
      "====== s011 ======\n",
      "====== s012 ======\n",
      "====== s013 ======\n",
      "====== s014 ======\n",
      "====== s015 ======\n",
      "====== s016 ======\n",
      "====== s017 ======\n",
      "====== s018 ======\n",
      "====== s019 ======\n",
      "====== s020 ======\n",
      "====== s021 ======\n",
      "====== s022 ======\n",
      "====== s023 ======\n",
      "====== s024 ======\n",
      "====== s025 ======\n",
      "====== s026 ======\n",
      "====== s027 ======\n",
      "====== s028 ======\n",
      "====== s029 ======\n",
      "====== s030 ======\n",
      "====== s031 ======\n",
      "====== s032 ======\n",
      "====== s033 ======\n",
      "====== s034 ======\n",
      "====== s035 ======\n",
      "====== s036 ======\n",
      "====== s037 ======\n",
      "====== s038 ======\n",
      "====== s039 ======\n",
      "====== s040 ======\n",
      "====== s041 ======\n",
      "====== s042 ======\n",
      "====== s043 ======\n",
      "====== s044 ======\n",
      "====== s045 ======\n",
      "====== s046 ======\n",
      "====== s047 ======\n",
      "====== s048 ======\n",
      "====== s049 ======\n",
      "====== s050 ======\n",
      "====== s051 ======\n",
      "====== s052 ======\n",
      "====== s053 ======\n",
      "====== s054 ======\n",
      "====== s055 ======\n",
      "====== s056 ======\n",
      "====== s057 ======\n",
      "====== s058 ======\n",
      "====== s059 ======\n",
      "====== s060 ======\n",
      "====== s061 ======\n",
      "====== s062 ======\n",
      "====== s063 ======\n",
      "====== s064 ======\n",
      "====== s065 ======\n",
      "====== s066 ======\n",
      "====== s067 ======\n",
      "====== s068 ======\n",
      "====== s069 ======\n",
      "====== s070 ======\n",
      "====== s071 ======\n",
      "====== s072 ======\n",
      "====== s073 ======\n",
      "====== s074 ======\n",
      "====== s075 ======\n",
      "====== s076 ======\n",
      "====== s077 ======\n",
      "====== s078 ======\n",
      "====== s079 ======\n",
      "====== s080 ======\n",
      "====== s081 ======\n",
      "====== s082 ======\n",
      "====== s083 ======\n",
      "====== s084 ======\n",
      "====== s085 ======\n",
      "====== s086 ======\n",
      "====== s087 ======\n",
      "====== s088 ======\n",
      "====== s089 ======\n",
      "====== s090 ======\n",
      "====== s091 ======\n",
      "====== s092 ======\n",
      "====== s093 ======\n",
      "====== s094 ======\n",
      "====== s095 ======\n",
      "====== s096 ======\n",
      "====== s097 ======\n",
      "====== s098 ======\n",
      "====== s099 ======\n",
      "====== s100 ======\n",
      "====== s101 ======\n",
      "====== s102 ======\n",
      "====== s103 ======\n",
      "====== s104 ======\n",
      "====== s105 ======\n",
      "====== s106 ======\n",
      "====== s107 ======\n",
      "====== s108 ======\n",
      "====== s109 ======\n",
      "====== s110 ======\n",
      "====== s111 ======\n"
     ]
    }
   ],
   "source": [
    "from pmdarima.arima import auto_arima\n",
    "\n",
    "fcts_arima_list = list()\n",
    "# all_solvers = ['lbfgs','newton','bfgs','nm','cg','ncg']\n",
    "solver = 'newton'\n",
    "for id_series in id_series_all:\n",
    "    print(\"======\", id_series, \"======\")\n",
    "    y = DT_train[id_series]\n",
    "    model = auto_arima(\n",
    "        y,\n",
    "        d=0,\n",
    "        start_p=0,\n",
    "        max_p=2,\n",
    "        start_q=0,\n",
    "        max_q=2,\n",
    "        D=0,\n",
    "        start_P=0,\n",
    "        max_P=2,\n",
    "        start_Q=0,\n",
    "        max_Q=2,\n",
    "        m=7,\n",
    "        trace=False,\n",
    "        error_action=\"ignore\",\n",
    "        suppress_warnings=True,\n",
    "        seasonal=True,\n",
    "        solver=solver\n",
    "    )\n",
    "    f_arima = model.predict(HORIZON)\n",
    "    f_arima.name = id_series\n",
    "    fcts_arima_list.append(f_arima)\n",
    "\n",
    "fcts_arima = pd.concat(fcts_arima_list, axis=1)\n",
    "\n",
    "kaggle_submission_arima = kaggle_forecasts(fcts_arima)\n",
    "kaggle_submission_arima.to_csv(\"../work/submission_arima.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential Smoothing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== s001 ======\n",
      "====== s002 ======\n",
      "====== s003 ======\n",
      "====== s004 ======\n",
      "====== s005 ======\n",
      "====== s006 ======\n",
      "====== s007 ======\n",
      "====== s008 ======\n",
      "====== s009 ======\n",
      "====== s010 ======\n",
      "====== s011 ======\n",
      "====== s012 ======\n",
      "====== s013 ======\n",
      "====== s014 ======\n",
      "====== s015 ======\n",
      "====== s016 ======\n",
      "====== s017 ======\n",
      "====== s018 ======\n",
      "====== s019 ======\n",
      "====== s020 ======\n",
      "====== s021 ======\n",
      "====== s022 ======\n",
      "====== s023 ======\n",
      "====== s024 ======\n",
      "====== s025 ======\n",
      "====== s026 ======\n",
      "====== s027 ======\n",
      "====== s028 ======\n",
      "====== s029 ======\n",
      "====== s030 ======\n",
      "====== s031 ======\n",
      "====== s032 ======\n",
      "====== s033 ======\n",
      "====== s034 ======\n",
      "====== s035 ======\n",
      "====== s036 ======\n",
      "====== s037 ======\n",
      "====== s038 ======\n",
      "====== s039 ======\n",
      "====== s040 ======\n",
      "====== s041 ======\n",
      "====== s042 ======\n",
      "====== s043 ======\n",
      "====== s044 ======\n",
      "====== s045 ======\n",
      "====== s046 ======\n",
      "====== s047 ======\n",
      "====== s048 ======\n",
      "====== s049 ======\n",
      "====== s050 ======\n",
      "====== s051 ======\n",
      "====== s052 ======\n",
      "====== s053 ======\n",
      "====== s054 ======\n",
      "====== s055 ======\n",
      "====== s056 ======\n",
      "====== s057 ======\n",
      "====== s058 ======\n",
      "====== s059 ======\n",
      "====== s060 ======\n",
      "====== s061 ======\n",
      "====== s062 ======\n",
      "====== s063 ======\n",
      "====== s064 ======\n",
      "====== s065 ======\n",
      "====== s066 ======\n",
      "====== s067 ======\n",
      "====== s068 ======\n",
      "====== s069 ======\n",
      "====== s070 ======\n",
      "====== s071 ======\n",
      "====== s072 ======\n",
      "====== s073 ======\n",
      "====== s074 ======\n",
      "====== s075 ======\n",
      "====== s076 ======\n",
      "====== s077 ======\n",
      "====== s078 ======\n",
      "====== s079 ======\n",
      "====== s080 ======\n",
      "====== s081 ======\n",
      "====== s082 ======\n",
      "====== s083 ======\n",
      "====== s084 ======\n",
      "====== s085 ======\n",
      "====== s086 ======\n",
      "====== s087 ======\n",
      "====== s088 ======\n",
      "====== s089 ======\n",
      "====== s090 ======\n",
      "====== s091 ======\n",
      "====== s092 ======\n",
      "====== s093 ======\n",
      "====== s094 ======\n",
      "====== s095 ======\n",
      "====== s096 ======\n",
      "====== s097 ======\n",
      "====== s098 ======\n",
      "====== s099 ======\n",
      "====== s100 ======\n",
      "====== s101 ======\n",
      "====== s102 ======\n",
      "====== s103 ======\n",
      "====== s104 ======\n",
      "====== s105 ======\n",
      "====== s106 ======\n",
      "====== s107 ======\n",
      "====== s108 ======\n",
      "====== s109 ======\n",
      "====== s110 ======\n",
      "====== s111 ======\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "fcts_expsmooth_list = list()\n",
    "for id_series in id_series_all:\n",
    "    print(\"======\", id_series, \"======\")\n",
    "    y = DT_train[id_series]\n",
    "    # Fit the model\n",
    "    model = sm.tsa.ExponentialSmoothing(y, seasonal='add', seasonal_periods=7).fit()\n",
    "\n",
    "    # Make forecasts\n",
    "    f_expsmooth = model.forecast(HORIZON)\n",
    "    f_expsmooth.name = id_series\n",
    "    fcts_expsmooth_list.append(f_expsmooth)\n",
    "\n",
    "fcts_expsmooth = pd.concat(fcts_expsmooth_list, axis=1)\n",
    "\n",
    "kaggle_submission_arima = kaggle_forecasts(fcts_expsmooth)\n",
    "kaggle_submission_arima.to_csv(\"../work/submission_expsmooth.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM without extracting any features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== s001 ======\n",
      "====== s002 ======\n",
      "====== s003 ======\n",
      "====== s004 ======\n",
      "====== s005 ======\n",
      "====== s006 ======\n",
      "====== s007 ======\n",
      "====== s008 ======\n",
      "====== s009 ======\n",
      "====== s010 ======\n",
      "====== s011 ======\n",
      "====== s012 ======\n",
      "====== s013 ======\n",
      "====== s014 ======\n",
      "====== s015 ======\n",
      "====== s016 ======\n",
      "====== s017 ======\n",
      "====== s018 ======\n",
      "====== s019 ======\n",
      "====== s020 ======\n",
      "====== s021 ======\n",
      "====== s022 ======\n",
      "====== s023 ======\n",
      "====== s024 ======\n",
      "====== s025 ======\n",
      "====== s026 ======\n",
      "====== s027 ======\n",
      "====== s028 ======\n",
      "====== s029 ======\n",
      "====== s030 ======\n",
      "====== s031 ======\n",
      "====== s032 ======\n",
      "====== s033 ======\n",
      "====== s034 ======\n",
      "====== s035 ======\n",
      "====== s036 ======\n",
      "====== s037 ======\n",
      "====== s038 ======\n",
      "====== s039 ======\n",
      "====== s040 ======\n",
      "====== s041 ======\n",
      "====== s042 ======\n",
      "====== s043 ======\n",
      "====== s044 ======\n",
      "====== s045 ======\n",
      "====== s046 ======\n",
      "====== s047 ======\n",
      "====== s048 ======\n",
      "====== s049 ======\n",
      "====== s050 ======\n",
      "====== s051 ======\n",
      "====== s052 ======\n",
      "====== s053 ======\n",
      "====== s054 ======\n",
      "====== s055 ======\n",
      "====== s056 ======\n",
      "====== s057 ======\n",
      "====== s058 ======\n",
      "====== s059 ======\n",
      "====== s060 ======\n",
      "====== s061 ======\n",
      "====== s062 ======\n",
      "====== s063 ======\n",
      "====== s064 ======\n",
      "====== s065 ======\n",
      "====== s066 ======\n",
      "====== s067 ======\n",
      "====== s068 ======\n",
      "====== s069 ======\n",
      "====== s070 ======\n",
      "====== s071 ======\n",
      "====== s072 ======\n",
      "====== s073 ======\n",
      "====== s074 ======\n",
      "====== s075 ======\n",
      "====== s076 ======\n",
      "====== s077 ======\n",
      "====== s078 ======\n",
      "====== s079 ======\n",
      "====== s080 ======\n",
      "====== s081 ======\n",
      "====== s082 ======\n",
      "====== s083 ======\n",
      "====== s084 ======\n",
      "====== s085 ======\n",
      "====== s086 ======\n",
      "====== s087 ======\n",
      "====== s088 ======\n",
      "====== s089 ======\n",
      "====== s090 ======\n",
      "====== s091 ======\n",
      "====== s092 ======\n",
      "====== s093 ======\n",
      "====== s094 ======\n",
      "====== s095 ======\n",
      "====== s096 ======\n",
      "====== s097 ======\n",
      "====== s098 ======\n",
      "====== s099 ======\n",
      "====== s100 ======\n",
      "====== s101 ======\n",
      "====== s102 ======\n",
      "====== s103 ======\n",
      "====== s104 ======\n",
      "====== s105 ======\n",
      "====== s106 ======\n",
      "====== s107 ======\n",
      "====== s108 ======\n",
      "====== s109 ======\n",
      "====== s110 ======\n",
      "====== s111 ======\n"
     ]
    }
   ],
   "source": [
    "prediction_days = HORIZON\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "fcts_lstm_list = list()\n",
    "for id_series in id_series_all:\n",
    "    print(\"======\", id_series, \"======\")\n",
    "    data = DT_train[id_series]\n",
    "\n",
    "    scaled_data = scaler.fit_transform(data.values.reshape(-1,1))\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    for x in range(prediction_days, len(scaled_data)):\n",
    "        x_train.append(scaled_data[x - prediction_days:x, 0])\n",
    "        y_train.append(scaled_data[x, 0])\n",
    "        \n",
    "    x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "\n",
    "    # Define the model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, input_shape=(28, 1)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.build()\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    model.fit(x_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose = 0)\n",
    "\n",
    "    model_inputs = data[len(data) - 2*prediction_days:].values\n",
    "    model_inputs = model_inputs.reshape(-1,1)\n",
    "    model_inputs = scaler.transform(model_inputs)\n",
    "\n",
    "    x_test = []\n",
    "    for x in range(prediction_days, len(model_inputs)):\n",
    "        x_test.append(model_inputs[x-prediction_days:x, 0])\n",
    "\n",
    "    x_test = np.array(x_test)\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1] ,1))\n",
    "\n",
    "    predicted_prices = model.predict(x_test, verbose=0)\n",
    "    predicted_prices = scaler.inverse_transform(predicted_prices)\n",
    "\n",
    "    f_lstm = pd.Series(list(predicted_prices),index=pd.date_range(\"19980323\", periods=28))\n",
    "    for i in range(len(f_lstm)):\n",
    "        f_lstm[i]= f_lstm[i][0]\n",
    "    f_lstm.name = id_series\n",
    "    fcts_lstm_list.append(f_lstm)\n",
    "\n",
    "fcts_lstm = pd.concat(fcts_lstm_list, axis=1)\n",
    "\n",
    "kaggle_submission_arima = kaggle_forecasts(fcts_lstm)\n",
    "kaggle_submission_arima.to_csv(\"../work/submission_lstm.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e48c41d80c68f25f83b1c1ab438a3679917a092eee9939eeba3728184a1b771"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
